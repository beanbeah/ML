{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from qbstyles import mpl_style\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' Theorem\n",
    "Bayes Theorem states the following relationship, \n",
    "$$\\large P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$$\n",
    "\n",
    "That is, given class variable $y$ and dependent feature vector $x_1$ through $x_n$:\n",
    "$$\\large P(y|x_1,...,x_n) = \\frac{P(y)P(x_1,...,x_n|y)}{P(x_1,...,x_n)}$$\n",
    "\n",
    "<br> Notation stolen from: <br>\n",
    "https://scikit-learn.org/stable/modules/naive_bayes.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive\n",
    "By definition of conditional probability (and ignoring the denominator for the moment), \n",
    "$$\\large P(y)P(x_1,...,x_n | y) = P(x_1,...,x_n,y)$$\n",
    "Using the chain rule to decompose, \n",
    "$$\\large P(x_1,...x_n,y) = P(x_1|x_2,...,x_n,y)P(x_2|x_3,...,x_n,y)...P(x_{n-1}|x_n,y)P(x_n|y)P(y)$$\n",
    "While this is correct, this is not entirely useful to us. A term such as $P(x_1|x_2,...,x_n,y) has so many conditions that it is not possible to calculate its probability accurately\n",
    "<br><br>\n",
    "Let us instead make the \"naive\" assumption that data is conditionally independant (of each other) given the class label. Naive Bayes is thus named naive for assumping independence where it might not actually exist. With this simplification, we can rewrite the chain rule probabilities using the conditional independence assumption. \n",
    "$$\n",
    "\\begin{align*}\n",
    "P(x_1,...x_n,y) &= P(x_1|x_2,...,x_n,y)P(x_2|x_3,...,x_n,y)...P(x_{n-1}|x_n,y)P(x_n|y)P(y) \\\\\n",
    "& \\approx P(x_1|y)P(x_2|y)...P(x_{n-1}|y)P(x_n|y)P(y) \\\\\n",
    "&= P(y) \\prod_{i=1}^{n}P(x_i | y)\n",
    "\\end{align*}\n",
    "$$\n",
    "<br> Explanation derived from: <br>\n",
    "https://courses.cs.washington.edu/courses/cse312/18sp/lectures/naive-bayes/naivebayesnotes.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together\n",
    "With this simplification, our relationship can be simplified to\n",
    "$$\\large P(y|x_1,...,x_n) = \\frac{P(y) \\prod_{i=1}^{n}P(x_i | y)}{P(x_1,...,x_n)}$$\n",
    "\n",
    "We can futher simplify this by dropping the denominator $P(x_1,...,x_n)$. This is possible because we will be computing $\\frac{P(y) \\prod_{i=1}^{n}P(x_i | y)}{P(x_1,...,x_n)}$ for each possible class, but $P(x_1,...,x_n)$ is constant regardless of class. Here, we are always asking about the most likely class for the same features, which must have the same probability $P(x_1,...,x_n)$. Therefore, \n",
    "$$\\large P(y|x_1,...,x_n) \\propto P(y) \\prod_{i=1}^{n}P(x_i | y)$$\n",
    "\n",
    "In other words, \n",
    "$$\\large \\hat{y} = \\underset{y}{\\mathrm{argmax}} \\space P(y) \\prod_{i=1}^{n}P(x_i | y)$$ \n",
    "\n",
    "\n",
    "* A quick notation note, $\\space \\large \\hat{} \\space$ means \"our estimate of the correct class\"\n",
    "* $P(x_i | y)$ is the likelihood\n",
    "* $P(y)$ is the priror probability of the class\n",
    "* argmax is an operation that finds the argument that gives the maximum value from a target function. Argmax is most commonly used in machine learning for finding the class with the largest predicted probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log space\n",
    "To prevent floating point underflows (multiplying a set of small probabilities will likely result in floating point underflow, where the product will become too small to represent and be replaced with 0) and to increase speed, Naive Bayes calculations are commonly done in log space. Recall, $log(ab) = log(a) + log(b)$. Therefore, \n",
    "$$\\large \\hat{y} = \\underset{y}{\\mathrm{argmax}} \\space log(P(y)) + \\sum_{i=1}^{n}log(P(x_i|c))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Other Notation...\n",
    "$$\\large C_{NB} = C_{MAP} = \\underset{c_j \\in C}{\\mathrm{argmax}} \\space log(P(C_j)) +\\sum_{i \\in positions}log(P(x_i|c_j))$$\n",
    "where C is class (Note, sometimes $C_{NB}$ is also $C_{MAP}$ where MAP is \"maximum a posteriori\", i.e the most likely class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Data\n",
    "The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_i|y)$. We give some possibilities below:\n",
    "- For real values, we can use the Gaussian distribution:<br>\n",
    "$$\n",
    "    \\large p(\\boldsymbol{x} | y = c, \\boldsymbol{\\theta}) = \\prod_{i=1}^{D} \\mathcal{N}(x_i| \\mu_{ic}, \\sigma_{ic}^2)\n",
    "$$<br>\n",
    "- For binary values, we can use a Bernouilli distribution, where $\\mu_{ic}$ is the probability that feature $i$ occurs in class $c$:<br>\n",
    "$$\n",
    "    \\large p(\\boldsymbol{x} | y = c, \\boldsymbol{\\theta}) = \\prod_{i=1}^{D} \\text{Ber}(x_i | \\mu_{ic}) \n",
    "$$<br>\n",
    "- For categorical features, we can use a Multinouilli distribution, where $\\boldsymbol{\\mu}_{ic}$ is an histogram over the possible values for $x_i$ in class $c$:<br>\n",
    "$$\n",
    "    \\large p(\\boldsymbol{x} | y = c, \\boldsymbol{\\theta}) = \\prod_{i=1}^{D}\\text{Cat}(x_i | \\boldsymbol{\\mu}_{ic})\n",
    "$$<br>\n",
    "\n",
    "Source: http://blog.axelmendoza.fr/naive-bayes/alcohol/pytorch/eda/from-scratch/2020/09/17/Naive-Bayes-Classifier.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing\n",
    "In a natural language processing task, our goal is to classify messages as either spam or not-spam (also known as ham). As such, our distribution of data is **multinomial**. (Data here is typically represented as word vector counts, thus making the distribution multinomial) NOTE, the algorithm implemented here **assumes a multinomial distribution of data**.\n",
    "\n",
    "To estimate the probabilities, we first try the maximum likelihood estimate (MLE), which is simply the relative frequency and corresponds to the most likely value of each parameter given the trianing data. \n",
    "\n",
    "Let $N_y$ be the number of data in our training data with class $y$ and $N_{total}$ be the total number of data in our dataset. Then it follows that $$ \\hat{P}(y) = \\frac{N_y}{N_{total}}$$\n",
    "\n",
    "To learn the probability $P(x_i|y)$ of feature $i$ appearing in a sample belonging to class $y$, we can make use of relative frequency counting. That is, $$\\hat{P}(x_i|y) = \\frac{N_{yi}}{N_y}$$ where $N_{yi} = \\sum_{x \\in T}x_i$ is the number of times feature $i$ appears in a sample of class $y$ in the training set $T$, and $N_y = \\sum_{i=1}^{n}N_{yi}$ is the total count of all feature for class $y$\n",
    "\n",
    "\n",
    "Notation Note: we write $\\hat{P}$ for $P$ because we do not know the true values of the parameters $P(y)$ and $P(x_i|y)$, but rather we westimate them from the training set. \n",
    "\n",
    "Source: https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html (notation note)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "We however encouter a problem with MLE estimation. Consider a feature-class combination that did not occur in the training data. Take for example a feature $K$ that only occured in class $a$. The MLE estimates for other classes, say class $b$ would then be 0. $$\\hat{P}(K|b) = 0$$ In such a case, our conditional probablitiy will be zero for class $b$ because we are multiplying the conditional probabiltiies for all terms (ref above first equation). This problem is due to spareseness, the training data is never large enough to represent the frequency of rare events adequately. \n",
    "\n",
    "The solution? Smoothing! This simply adds $\\alpha$. We can rewrite our probability as follows: \n",
    "$$\\hat{P}(x_i|y) = \\frac{N_{yi} + \\alpha}{N_y +\\alpha n}$$ \n",
    "where $N_{yi} = \\sum_{x \\in T}x_i$ is the number of times feature $i$ appears in a sample of class $y$ in the training set $T$, and $N_y = \\sum_{i=1}^{n}N_{yi}$ is the total count of all feature for class $y$, and $n$ is the number of features (in text classification, the size of the vocabulary) \n",
    "\n",
    "Note, $\\alpha$ is a hyperparameter. Setting $\\alpha=1$ is called Laplace smoothing, while $\\alpha < 1$ is called Lidstone smoothing.\n",
    "\n",
    "Source: https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Algorithm\n",
    "In conclusion, these are the training steps:\n",
    "1) Group data according to the class label $c_i$\n",
    "2) Compute the $log$ of the prior probability $\\hat{P}(y_i)$ \n",
    "3) For each feature, compute the $log$ of $\\hat{P}(x_i|y)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Recall, the naive bayes classifier is defined as: P(y|x) = P(x|y)P(y)\n",
    "Note, P(y) and P(x|y) are calculated in the training phase\n",
    "We return the log of it to prevent floating point underflow (reference above)\n",
    "'''\n",
    "\n",
    "def train(X,y, alpha=1.0):\n",
    "    #number of samples\n",
    "    sampleCount = X.shape[0]\n",
    "\n",
    "    #group samples by class label\n",
    "    separated = [[x for x, t in zip(X, y) if t == c] for c in np.unique(y)]\n",
    "\n",
    "    #calculate prior probability for each class, i.e P(y) = N_y/N_total\n",
    "    classLogPrior = [np.log(len(i) / sampleCount) for i in separated] \n",
    "\n",
    "    #calculate P(x_i|y) with smoothing (we default to laplace smoothing, alpha = 1)\n",
    "    #first calculate frequency\n",
    "    count = np.array([np.array(i).sum(axis=0) for i in separated]) + alpha \n",
    "    #then calculate log probability \n",
    "    #[np.newaxis].T is simpy to transpore the array to allow for broadcasting.\n",
    "    featureLogProb = np.log(count / count.sum(axis=1)[np.newaxis].T) \n",
    "    \n",
    "    return classLogPrior, featureLogProb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Algorithm\n",
    "Recall, $$\\large \\hat{y} = \\underset{y}{\\mathrm{argmax}} \\space log(P(y)) + \\sum_{i=1}^{n}log(P(x_i|c))$$\n",
    "\n",
    "To predict on new samples, \n",
    "- Add the log of each class, $log(P(y))$ with:\n",
    "  - For each feature $k$:\n",
    "    - Add the $log$ conditional probabilities calculated during training, $log(P(x_k|y_i))$ where $x_k$ is the value of the input on feature $k$\n",
    "- Finally, return the highest probability $P(y_i|x)$ of all classes\n",
    "\n",
    "Adapted from: https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
    "<br> Also taken from: http://kenzotakahashi.github.io/naive-bayes-from-scratch-in-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, classLogPrior, featureLogProb):\n",
    "    #calculate P(x|y)P(y)\n",
    "    combinedLikelihood = [(featureLogProb * x).sum(axis=1) + classLogPrior for x in X]\n",
    "    #return the class with the highest probability\n",
    "    return np.argmax(combinedLikelihood, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithmic Analysis\n",
    "Training Time complexity of $O(nd+cd) = O(d)$ where:\n",
    "* c the number of classes, in this case 1 since its binary classification\n",
    "* n the number of instances/samples\n",
    "* d the number of dimensions (atrributes)\n",
    "* all it needs to do is computing the frequency of every feature value $d_i$ for each class.\n",
    "\n",
    "Prediction Time complexity of $O(cd) = O(d)$ (remember, its a binary classifier, so $c = 1$)\n",
    "* since you have to retrieve d feature values for each of the c classes. \n",
    "\n",
    "Space Complexity of $O(d)$\n",
    "* Note, only decision trees are more compact\n",
    "\n",
    "Source: https://www.inf.ed.ac.uk/teaching/courses/iaml/slides/naive-2x2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes/Others\n",
    "* Naive Bayes is very fast with low storage requirements\n",
    "* Robust to Irrelevant Features\n",
    "  * Irrevelevant features cancel each other without affecting results\n",
    "* Very good in domains with many equally important features\n",
    "* Optimal if the indepedence assumption hold\n",
    "\n",
    "Real-world stuff:\n",
    "* With enough data, classifier may not matter. (Brill and Banko on spelling correction graph)\n",
    "\n",
    "Tweaking Performance:\n",
    "* Domain-specific features and weights\n",
    "* Collapse Terms\n",
    "* Upweighting (Counting a word as if it occured twice)\n",
    "  * Title Words (Cohen & Singer 1996)\n",
    "  * First Sentence of each paragraph (Murata, 1999)\n",
    "  * In sentences that contain title words (Ko et al, 2002)\n",
    "\n",
    "Src: http://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf\n",
    "\n",
    "\n",
    "### Other Amazing Resources:\n",
    "https://web.stanford.edu/~jurafsky/slp3/4.pdf <br>\n",
    "https://cs229.stanford.edu/notes2021fall/cs229-notes2.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('3.8.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9fe690d5d2a41e80a39a1c951cfd946a8619f3304547969fd557fac12cd97ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
