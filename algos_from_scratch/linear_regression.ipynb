{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the equation of a straight line: $y=mx+c$. We are familar with $x$ being our variable, $c$ being our y-intercept and $m$ being our slope\n",
    "\n",
    "We can also represent this in the form of a matrix, making use of a linear combination.$$z = b + w_{1}x_{1} + w_{2}x_{2} + ... + w_{n}x_{n}$$ Here, $z$ is a vector in $\\real^m$, $b$ is the intercept term, and $w_{0}...w_{n}$ are weights (coefficients) to the vectors $x_{1}...x_{n}$.  \n",
    "\n",
    "That is, \n",
    "$$y = b + \\sum_{i=1}^{n}x_iw_i$$\n",
    "\n",
    "$z$ can also be represented using matrices. Let us represent our set of features $x$ as a $m\\times n$ matrix, and our weights as a $n \\times 1$ matrix.\n",
    "$$\\begin{bmatrix} z_1 \\\\ z_2 \\\\ \\vdots  \\\\ z_m \\end{bmatrix} = \\begin{bmatrix}x^1_1 & x^1_2 & \\ldots & x^1_n\\\\x^2_1 & x^2_2 & \\ldots & x^2_n\\\\\\vdots & \\vdots & \\ddots & \\vdots\\\\x^m_1 & x^m_2 & \\ldots & x^m_n\\end{bmatrix} \\cdot \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots  \\\\ w_n \\end{bmatrix} + b$$ \n",
    "Note, the dot product of an $m \\times n$ matrix and an $n \\times 1$ matrix is a $m \\times 1$ matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression in matrix notation:\n",
    "\n",
    "Notice that we use w0 as an intercept term, and thus we need to add a dummy dimension with value of “1” (x0) for all data points x. Thus, x here is on d+1 dimension. Think of it as the y-intercept term c in 2-dimension (y=mx+c).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "It should be noted that features are commonly expressed as $x$ with the output/classification/result as $y$. With reference to the above linear combinator expressed in the form of a dot product, each column is essentially 1 feature to be used in this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All LR models create a trend with the formula of a linear combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "SSE is sum of squared errors, That is, the sum of squared of vectors y-y(hat). \n",
    "\n",
    "Using sum of squares = x^Tx, \n",
    "\n",
    "y-y(hat) = y-Xw\n",
    "\n",
    "||y-Xw||^2 = (y-Xw)^T(y-Xw)\n",
    "\n",
    "https://aunnnn.github.io/ml-tutorial/html/blog_content/linear_regression/linear_regression_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, the SSE is a quadratic function, with a minima. Since our goal here is to reduce error to a minimum, we want to find the minimum point. We can do this by solving for derivatives with respect to weights being 0. \n",
    "\n",
    "\n",
    "Background knowledge: dy/dx gets the gradient of a curve at a point. \n",
    "We know that solving for dy/dx = 0 gives us our turning points, but because this is a quadratic function, there is only 1 turning point, which is a minima. Therefore, we will get our minimum point\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot Product --> Transpose --> Inner Product \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
